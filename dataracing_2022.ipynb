{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f762e5-10cc-4dcc-9356-ba0a0af1a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Racing competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dfcb20-2612-4b62-8c29-6560cdb55835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks\n",
    "\n",
    "2022.06.01 \n",
    "1. Standardisation of data - logarithm of numeric data\n",
    "2. Outlier detection\n",
    "3. Setting up the Colab Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5fc91a-1cce-4468-9869-7ad95d3513a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags\n",
    "regression, xgboost, catboost, lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee1976-847c-4138-b218-bc2cb7d8afb3",
   "metadata": {},
   "source": [
    "# Reference\n",
    "[xgboost basics](https://www.kaggle.com/code/carlmcbrideellis/an-introduction-to-xgboost-regression/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e64ee8-d3dc-456e-b596-b4e0b2cd4d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from numpy.random import normal\n",
    "from numpy import hstack\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "# utils\n",
    "from utils import tab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5ef76-8325-4671-8404-23d66dadd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#print all rows of a df in ipython shell \n",
    "pd.set_option('display.max_rows', None)\n",
    "#print all columns of a df in ipython shell \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option(\"display.precision\", 6)\n",
    "\n",
    "\n",
    "# optional\n",
    "pd.set_option('display.max_columns',100)\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d78388-acf0-4f25-b7ac-d38cf987d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "aktelh - aktivált időbeli elhatárolás\n",
    "arbevert - árbevétel elért\n",
    "arbevexp - árbevétel export\n",
    "befpue - befektetett pénzügyi eszközök\n",
    "begyeb - befektetett egyéb\n",
    "celtart - céltartalék\n",
    "eredadel - eredeti adózás előtt\n",
    "eredadoz - eredeti adózott\n",
    "eredpenz - eredeti pénz\n",
    "eredtar - eredeti tartozás\n",
    "ereduzem - eredeti uzemeltetés\n",
    "ertpapir - értékpapir\n",
    "hoskot01 - hosszúlejáratú kötelezettségek\n",
    "hoskot - hosszulejáratú kötelezettségek\n",
    "immat - immateriális javak\n",
    "jetok - jegyzett tőke\n",
    "kecs - keszlet értékcsökkenés\n",
    "keszl - készlet\n",
    "kovet - követelések \n",
    "letszam - létszám\n",
    "merlfoo - mérleg főösszeg\n",
    "penzeszk - pénzeszközök\n",
    "ranyag - ráfordítások anyag\n",
    "regyeb - ráforditások egyéb\n",
    "rovkot_01 -\n",
    "rovkot - rövid lejáratú kötelezettségek\n",
    "rszem - ráforditás személyzet\n",
    "sajattok - saját tőke\n",
    "targyie - tárgyi eszköz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f810b4-e218-4507-b9e2-eb284661c8fd",
   "metadata": {},
   "source": [
    "# Data Enginieering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f492606-69a9-497e-866e-920a9563b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load data\n",
    "print(\"1. Load data\")\n",
    "\n",
    "# path = 'e:\\PycharmProjects\\CompleteEDA'\n",
    "path = os.path.abspath(os.getcwd())\n",
    "train = 'train.csv'\n",
    "test = 'test.csv'\n",
    "\n",
    "path_to_train = os.path.join(path, 'data', train)\n",
    "\n",
    "path_to_test = os.path.join(path, 'data', test)\n",
    "\n",
    "df_train = pd.read_csv(path_to_train)\n",
    "df_test_orig = pd.read_csv(path_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d23ad8c-6c28-47a3-9a21-9e410cd29c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create df for TRAINING\n",
    "print(\"2. Create df for TRAINING\")\n",
    "df = df_train.copy()\n",
    "\n",
    "# 3. transform nas and erroneous big alakul_ev to 2013 \n",
    "print(\"3. Transform nas and erroneous big alakul_ev to 2013\")\n",
    "\n",
    "col = 'alakul_ev'\n",
    "df[col] = np.where((df.alakul_ev.isna()) |  (df.alakul_ev > 2016), 2013, df.alakul_ev)\n",
    "\n",
    "# 4. fill missing categorical values with the most frequent value\n",
    "print(\"4. Fill missing categorical values with the most frequent value\")\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# get columns with missing values\n",
    "columns_cat_missing_values = [column for column in df.columns if df[column].isna().sum() > 0]\n",
    "\n",
    "# fill with most frequent values\n",
    "for col in columns_cat_missing_values:\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    df[col] = imp.fit_transform(df[col].values.reshape(-1, 1))\n",
    "\n",
    "# 5. get numeric and object columns per years and divide numeric float ones by letszam\n",
    "print(\"5. Get numeric and object columns per years and divide numeric float ones by letszam\")\n",
    "\n",
    "cols_no_year = ['id', 'target_reg', 'alakul_ev']\n",
    "\n",
    "numeric_cols_2014 = [column for column in df.select_dtypes(exclude=['int64'], include=['float64']).columns if \"2014\" in column]\n",
    "numeric_cols_2014 = [item for item in numeric_cols_2014 if item not in  ['KATEGORIKUS_VALTOZO_2014', 'letszam_2014']]\n",
    "object_cols_2014 = [column for column in df.select_dtypes( include=['object']).columns if \"2014\" in column]\n",
    "\n",
    "numeric_cols_2015 = [column for column in df.select_dtypes(exclude=['int64'], include=['float64']).columns if \"2015\" in column]\n",
    "numeric_cols_2015 = [item for item in numeric_cols_2015 if item not in  ['KATEGORIKUS_VALTOZO_2015', 'letszam_2015']]\n",
    "object_cols_2015 = [column for column in df.select_dtypes(include=['object']).columns if \"2015\" in column]\n",
    "\n",
    "numeric_cols_2016 = [column for column in df.select_dtypes(exclude=['int64'], include=['float64']).columns if \"2016\" in column]\n",
    "numeric_cols_2016 = [item for item in numeric_cols_2016 if item not in  ['KATEGORIKUS_VALTOZO_2016', 'letszam_2016']]\n",
    "object_cols_2016 = [column for column in df.select_dtypes(include=['object']).columns if \"2016\" in column]\n",
    "\n",
    "\n",
    "# divide all numeric features by letszam expect of KATEGORIKUS_VALTOZO_x and letszam_x by year\n",
    "for col in numeric_cols_2014:\n",
    "    df[col] = np.where(df.letszam_2014!=0, df[col]/df.letszam_2014,0)\n",
    "    #df[col] = np.log(df[col])\n",
    "for col in numeric_cols_2015:\n",
    "    df[col] = np.where(df.letszam_2015!=0, df[col]/df.letszam_2015,0)\n",
    "    #df[col] = np.log(df[col])\n",
    "for col in numeric_cols_2016:\n",
    "    df[col] = np.where(df.letszam_2016!=0, df[col]/df.letszam_2016,0)\n",
    "    #df[col] = np.log(df[col])\n",
    "\n",
    "# divide target_reg by letszam_2016\n",
    "col = 'target_reg'\n",
    "df[col] = np.where(df.letszam_2016!=0, df[col]/df.letszam_2016,0)\n",
    "\n",
    "# 6. extract the categorical columns to add into the model \n",
    "print(\"6. Extract the categorical columns to add into the model\")\n",
    "\n",
    "cols_no_year = ['id', 'target_reg', 'alakul_ev']\n",
    "cols_kat_valt = ['KATEGORIKUS_VALTOZO_2014', 'KATEGORIKUS_VALTOZO_2015', 'KATEGORIKUS_VALTOZO_2016']\n",
    "\n",
    "columns_obj = [column for column in df.select_dtypes(include='object').columns if (\"2014\"  in column )|(\"2015\"  in column )|(\"2016\"  in column )]\n",
    "\n",
    "# 7. transform categorical columns into percentage ones\n",
    "print(\"7. Transform categorical columns into percentage ones\")\n",
    "\n",
    "for col in columns_obj:\n",
    "    # calculate the percentage \n",
    "    df_grouped = df[col].value_counts(normalize = True)\n",
    "    # get the index as column\n",
    "    df_grouped = df_grouped.reset_index()\n",
    "    # change the column names :: orig column and percent\n",
    "    percent_col = '%'+col\n",
    "    df_grouped.columns = [col, percent_col]\n",
    "    # merge the percent values to orig df\n",
    "    df = pd.merge(df, df_grouped, how='left')\n",
    "\n",
    "# 8. calculate percentage by count on alakul_ev\n",
    "print(\"8. Calculate percentage by count on alakul_ev\")\n",
    "\n",
    "col = 'alakul_ev'\n",
    "\n",
    "# calculate the percentage \n",
    "df_grouped = df[col].value_counts(normalize = True)\n",
    "# get the index as column\n",
    "df_grouped = df_grouped.reset_index()\n",
    "# change the column names :: orig column and percent\n",
    "percent_col = '%'+col\n",
    "df_grouped.columns = [col, percent_col]\n",
    "# merge the percent values to orig df\n",
    "df = pd.merge(df, df_grouped, how='left')\n",
    "\n",
    "# 9. create columns list for percentages columns\n",
    "print(\"9. Create columns list for percentages columns\")\n",
    "\n",
    "columns_obj_perc = [column for column in df.columns if \"%\" in column]\n",
    "\n",
    "# 10. apply PowerTransformer for numeric float and percentage float columns \n",
    "print(\"10. Apply PowerTransformer for numeric float and percentage float columns\")\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# numeric colums :: divided/letszam and categorical percentage columns\n",
    "numeric_cols_to_transform = numeric_cols_2014 + numeric_cols_2015 + numeric_cols_2016 + columns_obj_perc\n",
    "\n",
    "power_features = PowerTransformer(method=\"yeo-johnson\",  standardize=True).fit_transform(df[numeric_cols_to_transform].values)\n",
    "df[numeric_cols_to_transform] = pd.DataFrame(power_features, index=df.index, columns=df[numeric_cols_to_transform].columns)\n",
    "\n",
    "# 11. transform target_reg, saved it in transformed\n",
    "print(\"11. Transform target_reg, saved it in transformed\")\n",
    "\n",
    "col = ['target_reg']\n",
    "ptt = PowerTransformer(method=\"yeo-johnson\",  standardize=True)\n",
    "power_target_feature = ptt.fit_transform(df[col].values)\n",
    "print(\"Lamdas for target_reg :: {}\".format(ptt.lambdas_))\n",
    "df['transformed'] = pd.DataFrame(power_target_feature, index=df.index, columns=df[col].columns)\n",
    "\n",
    "# 12. model features selection\n",
    "print(\"12. model features selection\")\n",
    "\n",
    "model_columns = ['target_reg'] + ['transformed']+ cols_kat_valt + numeric_cols_2014 + numeric_cols_2015 + numeric_cols_2016 + columns_obj_perc\n",
    "\n",
    "df_model = df[model_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f3f72-a0fe-4d09-8962-bacaebccd506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create df for testing\n",
    "print(\"2. Create df for TESTING\")\n",
    "\n",
    "print(\"2. Create df for TRAINING\")\n",
    "df_test = df_test_orig.copy()\n",
    "\n",
    "# 3. transform nas and erroneous big alakul_ev to 2013 \n",
    "print(\"3. Transform nas and erroneous big alakul_ev to 2013\")\n",
    "\n",
    "col = 'alakul_ev'\n",
    "df_test[col] = np.where((df_test.alakul_ev.isna()) |  (df_test.alakul_ev > 2016), 2013, df_test.alakul_ev)\n",
    "\n",
    "# 4. fill missing categorical values with the most frequent value\n",
    "print(\"4. Fill missing categorical values with the most frequent value\")\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# get columns with missing values\n",
    "columns_cat_missing_values = [column for column in df_test.columns if df_test[column].isna().sum() > 0]\n",
    "\n",
    "# fill with most frequent values\n",
    "for col in columns_cat_missing_values:\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    df_test[col] = imp.fit_transform(df_test[col].values.reshape(-1, 1))\n",
    "\n",
    "# 5. get numeric and object columns per years and divide numeric float ones by letszam\n",
    "print(\"5. Get numeric and object columns per years and divide numeric float ones by letszam\")\n",
    "\n",
    "cols_no_year = ['id', 'target_reg', 'alakul_ev']\n",
    "\n",
    "numeric_cols_2014 = [column for column in df_test.select_dtypes(exclude=['int64'], include=['float64']).columns if \"2014\" in column]\n",
    "numeric_cols_2014 = [item for item in numeric_cols_2014 if item not in  ['KATEGORIKUS_VALTOZO_2014', 'letszam_2014']]\n",
    "object_cols_2014 = [column for column in df_test.select_dtypes( include=['object']).columns if \"2014\" in column]\n",
    "\n",
    "numeric_cols_2015 = [column for column in df_test.select_dtypes(exclude=['int64'], include=['float64']).columns if \"2015\" in column]\n",
    "numeric_cols_2015 = [item for item in numeric_cols_2015 if item not in  ['KATEGORIKUS_VALTOZO_2015', 'letszam_2015']]\n",
    "object_cols_2015 = [column for column in df_test.select_dtypes(include=['object']).columns if \"2015\" in column]\n",
    "\n",
    "numeric_cols_2016 = [column for column in df_test.select_dtypes(exclude=['int64'], include=['float64']).columns if \"2016\" in column]\n",
    "numeric_cols_2016 = [item for item in numeric_cols_2016 if item not in  ['KATEGORIKUS_VALTOZO_2016', 'letszam_2016']]\n",
    "object_cols_2016 = [column for column in df_test.select_dtypes(include=['object']).columns if \"2016\" in column]\n",
    "\n",
    "\n",
    "# divide all numeric features by letszam expect of KATEGORIKUS_VALTOZO_x and letszam_x by year\n",
    "for col in numeric_cols_2014:\n",
    "    df_test[col] = np.where(df_test.letszam_2014!=0, df_test[col]/df_test.letszam_2014,0)\n",
    "    #df[col] = np.log(df[col])\n",
    "for col in numeric_cols_2015:\n",
    "    df_test[col] = np.where(df_test.letszam_2015!=0, df_test[col]/df_test.letszam_2015,0)\n",
    "    #df[col] = np.log(df[col])\n",
    "for col in numeric_cols_2016:\n",
    "    df_test[col] = np.where(df_test.letszam_2016!=0, df_test[col]/df_test.letszam_2016,0)\n",
    "    #df[col] = np.log(df[col])\n",
    "\n",
    "\n",
    "# 6. extract the categorical columns to add into the model \n",
    "print(\"6. Extract the categorical columns to add into the model\")\n",
    "\n",
    "cols_no_year = ['id', 'target_reg', 'alakul_ev']\n",
    "cols_kat_valt = ['KATEGORIKUS_VALTOZO_2014', 'KATEGORIKUS_VALTOZO_2015', 'KATEGORIKUS_VALTOZO_2016']\n",
    "\n",
    "columns_obj = [column for column in df_test.select_dtypes(include='object').columns if (\"2014\"  in column )|(\"2015\"  in column )|(\"2016\"  in column )]\n",
    "\n",
    "# 7. transform categorical columns into percentage ones\n",
    "print(\"7. Transform categorical columns into percentage ones\")\n",
    "\n",
    "for col in columns_obj:\n",
    "    # calculate the percentage \n",
    "    df_grouped = df_test[col].value_counts(normalize = True)\n",
    "    # get the index as column\n",
    "    df_grouped = df_grouped.reset_index()\n",
    "    # change the column names :: orig column and percent\n",
    "    percent_col = '%'+col\n",
    "    df_grouped.columns = [col, percent_col]\n",
    "    # merge the percent values to orig df\n",
    "    df_test = pd.merge(df_test, df_grouped, how='left')\n",
    "\n",
    "# 8. calculate percentage by count on alakul_ev\n",
    "print(\"8. Calculate percentage by count on alakul_ev\")\n",
    "\n",
    "col = 'alakul_ev'\n",
    "\n",
    "# calculate the percentage \n",
    "df_grouped = df_test[col].value_counts(normalize = True)\n",
    "# get the index as column\n",
    "df_grouped = df_grouped.reset_index()\n",
    "# change the column names :: orig column and percent\n",
    "percent_col = '%'+col\n",
    "df_grouped.columns = [col, percent_col]\n",
    "# merge the percent values to orig df\n",
    "df_test = pd.merge(df_test, df_grouped, how='left')\n",
    "\n",
    "# 9. create columns list for percentages columns\n",
    "print(\"9. Create columns list for percentages columns\")\n",
    "\n",
    "columns_obj_perc = [column for column in df_test.columns if \"%\" in column]\n",
    "\n",
    "# 10. apply PowerTransformer for numeric float and percentage float columns \n",
    "print(\"10. Apply PowerTransformer for numeric float and percentage float columns\")\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# numeric colums :: divided/letszam and categorical percentage columns\n",
    "numeric_cols_to_transform = numeric_cols_2014 + numeric_cols_2015 + numeric_cols_2016 + columns_obj_perc\n",
    "\n",
    "power_features = PowerTransformer(method=\"yeo-johnson\",  standardize=True).fit_transform(df_test[numeric_cols_to_transform].values)\n",
    "df_test[numeric_cols_to_transform] = pd.DataFrame(power_features, index=df_test.index, columns=df_test[numeric_cols_to_transform].columns)\n",
    "\n",
    "# 12. model features selection\n",
    "print(\"12. model features selection\")\n",
    "\n",
    "model_columns = cols_kat_valt + numeric_cols_2014 + numeric_cols_2015 + numeric_cols_2016 + columns_obj_perc\n",
    "\n",
    "df_test = df_test[model_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec1ea83-2317-461b-a59c-9222c2e63303",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f542c3c3-f87d-44d1-9550-7dc10b2d6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "# path = 'e:\\PycharmProjects\\CompleteEDA'\n",
    "path = os.path.abspath(os.getcwd())\n",
    "train = 'train.csv'\n",
    "test = 'test.csv'\n",
    "\n",
    "path_to_train = os.path.join(path, 'data', train)\n",
    "\n",
    "path_to_test = os.path.join(path, 'data', test)\n",
    "\n",
    "df_orig = pd.read_csv(path_to_train)\n",
    "df_test = pd.read_csv(path_to_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430020e6-79e4-4935-800b-34aad37d86ff",
   "metadata": {},
   "source": [
    "# Quick checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b47589-1130-4340-845d-9a14d041b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2878a4aa-5261-492f-a1f0-b0acf771c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36395fbf-d6b8-4cde-aa77-5eb3d91ab449",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b4e62-1434-4e16-aea3-fd1ac7f028b9",
   "metadata": {},
   "source": [
    "# Create df for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b878c-2d6e-4ac3-9148-007a4ff9a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity\n",
    "df = df_orig.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33941ab-237b-40f5-951a-a226653cf0cb",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dfb350-e178-4a16-97be-5d81b2caadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print missing and unique values in tabular format\n",
    "from tabulate import tabulate\n",
    "def tab_data(df):\n",
    "    headers = ['Column', 'Null Count', 'Unique Count']\n",
    "    meta_list = []\n",
    "    cols = [i for i in df.columns]\n",
    "    for col in cols:\n",
    "        temp = []\n",
    "        temp.append(col)\n",
    "        temp.append(df[col].isna().sum())\n",
    "        temp.append(df[col].nunique())\n",
    "        meta_list.append(temp)\n",
    "    print(tabulate(meta_list, headers, tablefmt='rst'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e227cae8-a631-4843-81cd-49e6b1aa4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing and unique values\n",
    "tab_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e262a919-d85b-4ca9-898f-758317b46fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with alakul_ev - check for errors\n",
    "df_orig[(df_orig.alakul_ev.isna()) |  (df_orig.alakul_ev > 2016)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651b6b7-02b8-46d4-8b82-f0f639530ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform nas and erroneous big alakul_ev to 2013 \n",
    "#df_ = df.copy()\n",
    "col = 'alakul_ev'\n",
    "df[col] = np.where((df.alakul_ev.isna()) |  (df.alakul_ev > 2016), 2013, df.alakul_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196b913-ab4b-402f-9518-a9fb8a84071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check alakul ev\n",
    "df[col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c1b20c-e177-4881-9595-4e9f7450c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with the most frequent value\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# get columns with missing values\n",
    "columns_cat_missing_values = [column for column in df.columns if df[column].isna().sum() > 0]\n",
    "\n",
    "# fill with most frequent values\n",
    "for col in columns_cat_missing_values:\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    df[col] = imp.fit_transform(df[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f4671-6fdf-4b2c-8a6a-cc760b844bc5",
   "metadata": {},
   "source": [
    "# Feature enginieering\n",
    "\n",
    "Initial attempt             \n",
    "1. ertek - arbevetel/letszam              \n",
    "2. jövedelem - arbevétel/személyi ráforditás                \n",
    "\n",
    "Filling the missing values                  \n",
    "alakul_ev with 2013                    \n",
    "categorical features with the most frequent one                    \n",
    "\n",
    "Actual feature enginieering               \n",
    "transform categorical values by transforming counts of categoricals to percentage except of KATEGORIKUS_VALTOZO_201x and letszam_201x as          \n",
    "letszam was used in diving the numerical features           \n",
    "divide all the numeric columns by 'letszam' and omit letszam from the features                     \n",
    "apply PowerTransformer(method=\"yeo-johnson\") for all the numeric features               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25a8a7-eeb3-4f7e-b18c-905854bd0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get numeric and object columns per years :: miscellous\n",
    "cols_no_year = ['id', 'target_reg', 'alakul_ev']\n",
    "\n",
    "numeric_cols_2014 = [column for column in df.select_dtypes(exclude=['int64'], include=['float64']).columns if \"2014\" in column]\n",
    "numeric_cols_2014 = [item for item in numeric_cols_2014 if item not in  ['KATEGORIKUS_VALTOZO_2014', 'letszam_2014']]\n",
    "object_cols_2014 = [column for column in df.select_dtypes( include=['object']).columns if \"2014\" in column]\n",
    "\n",
    "numeric_cols_2015 = [column for column in df.select_dtypes(exclude=['int64'], include=['float64']).columns if \"2015\" in column]\n",
    "numeric_cols_2015 = [item for item in numeric_cols_2015 if item not in  ['KATEGORIKUS_VALTOZO_2015', 'letszam_2015']]\n",
    "object_cols_2015 = [column for column in df.select_dtypes(include=['object']).columns if \"2015\" in column]\n",
    "\n",
    "numeric_cols_2016 = [column for column in df.select_dtypes(exclude=['int64'], include=['float64']).columns if \"2016\" in column]\n",
    "numeric_cols_2016 = [item for item in numeric_cols_2016 if item not in  ['KATEGORIKUS_VALTOZO_2016', 'letszam_2016']]\n",
    "object_cols_2016 = [column for column in df.select_dtypes(include=['object']).columns if \"2016\" in column]\n",
    "\n",
    "\n",
    "# 2. jövedelem df - arbevétel/személyi ráforditás \n",
    "#df['szemely_2014'] = np.where(df.rszem_2014!=0, df.arbevert_2014/df.rszem_2014,0)\n",
    "#df['szemely_2015'] = np.where(df.rszem_2015!=0, df.arbevert_2014/df.rszem_2015,0)\n",
    "#df['szemely_2016'] = np.where(df.rszem_2016!=0, df.arbevert_2014/df.rszem_2016,0)\n",
    "\n",
    "# 3. divide all numeric features by letszam expect of KATEGORIKUS_VALTOZO_x and letszam_x by year\n",
    "\n",
    "for col in numeric_cols_2014:\n",
    "    df[col] = np.where(df.letszam_2014!=0, df[col]/df.letszam_2014,0.1)\n",
    "    #df[col] = np.log(df[col])\n",
    "for col in numeric_cols_2015:\n",
    "    df[col] = np.where(df.letszam_2015!=0, df[col]/df.letszam_2015,0.1)\n",
    "    #df[col] = np.log(df[col])\n",
    "for col in numeric_cols_2016:\n",
    "    df[col] = np.where(df.letszam_2016!=0, df[col]/df.letszam_2016,0.1)\n",
    "    #df[col] = np.log(df[col])\n",
    "\n",
    "# 3. divide target_reg by letszam_2016\n",
    "col = 'target_reg'\n",
    "df[col] = np.where(df.letszam_2016!=0, df[col]/df.letszam_2016,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b5865c-ad17-4e64-a48f-9c1338934c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check\n",
    "\n",
    "df[numeric_cols_2016].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69467b70-7c4b-451a-b112-f465541b6519",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ca086c-f596-43d1-8f4e-98a205ed8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the largest and smallest values\n",
    "\n",
    "df[['letszam_2016', 'target_reg']].nlargest(10, \"letszam_2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d141ba4-40ae-400d-9efb-2210b83c78d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['letszam_2016', 'target_reg']].nsmallest(10, \"letszam_2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259cb16e-f8fd-49aa-837f-c1e03542be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starnge values: target_reg < 0\n",
    "\n",
    "df[df.target_reg < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1109255-798b-4a11-8abb-1292b4a11434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers - target_reg 0 values\n",
    "df_ = df[df.target_reg < 0] \n",
    "df_0 = df[df.target_reg == 0] \n",
    "df_target = df[df.target_reg > 0]\n",
    "df_or_not_null = df[(df.target_reg == 0) | (df.arbevexp_2014 == 0 ) | (df.arbevexp_2015 == 0) | (df.arbevexp_2016 == 0) ]\n",
    "df_all_0 = df[(df.target_reg <= 0) & (df.arbevexp_2014 <= 0 ) & (df.arbevexp_2015 <= 0) & (df.arbevexp_2016 <= 0) ]\n",
    "df_not_all_0 = df[~((df.target_reg <= 0) & (df.arbevexp_2014 <= 0 ) & (df.arbevexp_2015 <= 0) & (df.arbevexp_2016 <= 0)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57157b9-58f4-4c95-9d44-9329db1d7a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_reg 0 values analysis\n",
    "\n",
    "print('NR of all elements :: {}'.format(len(df)))\n",
    "print('NR of negative targets :: {}'.format(len(df_)))\n",
    "print('NR of 0 targets :: {}'.format(len(df_0)))\n",
    "print('NR of pozitiv targets :: {}'.format(len(df_target)))\n",
    "print('NR of either 1 years export is 0 :: {}'.format(len(df_or_not_null)))\n",
    "print('NR of export for all years is 0 :: {}'.format(len(df_all_0)))\n",
    "print('NR of export for all years is not 0 :: {}'.format(len(df_not_all_0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a89bf-d827-41c1-8771-76852b1b3ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick checks\n",
    "columns = ['alakul_ev', 'letszam_2014','arbevert_2014','arbevexp_2014', 'letszam_2015', 'arbevexp_2015', 'letszam_2016', 'arbevexp_2016', 'target_reg' ]\n",
    "df_not_all_0[columns].sort_values(by='ertek_2014', ascending=False).sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7854289-1409-40d3-bb1b-cac38bb80af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_not_all_0[df.target_reg ==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac59c3-ae9b-48ee-858c-3961a67035f0",
   "metadata": {},
   "source": [
    "# Distribution of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea066f-8b21-481f-a35b-0fb9b4092c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758aefc-8ceb-4b0b-8666-a7ebb13d48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1e662-96b9-4e6a-b692-a9586b1a85dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise categorical columns\n",
    "columns = columns_2014_obj\n",
    "for column in columns_2014_obj:\n",
    "    if df[column].nunique() < 30:\n",
    "        sns.countplot(y=column, data=df)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d63d6b3-9812-47e4-9567-791e7ca51e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check of categorical columns\n",
    "df.SZEKHELY_IR_SZAM_2014.value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a0bd3e-0ee1-4c19-9cdc-4727a1449c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check of categorical columns\n",
    "df.SZEKHELY_IR_SZAM_2014.value_counts().nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d174521-08ce-4def-8d4d-04d3e4717a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the categorical columns to add into the model \n",
    "#df = df_orig.copy()\n",
    "\n",
    "cols_no_year = ['id', 'target_reg', 'alakul_ev']\n",
    "cols_kat_valt = ['KATEGORIKUS_VALTOZO_2014', 'KATEGORIKUS_VALTOZO_2015', 'KATEGORIKUS_VALTOZO_2016']\n",
    "\n",
    "columns_obj = [column for column in df.select_dtypes(include='object').columns if (\"2014\"  in column )|(\"2015\"  in column )|(\"2016\"  in column )]\n",
    "print(\"All object columns:: \\n{}\".format(columns_obj)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7510e0-a26b-4cf2-90b5-89e538e57af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess categorical columns - transform them in percentage ones\n",
    "\n",
    "for col in columns_obj:\n",
    "    # calculate the percentage \n",
    "    df_grouped = df[col].value_counts(normalize = True)\n",
    "    # get the index as column\n",
    "    df_grouped = df_grouped.reset_index()\n",
    "    # change the column names :: orig column and percent\n",
    "    percent_col = '%'+col\n",
    "    df_grouped.columns = [col, percent_col]\n",
    "    # merge the percent values to orig df\n",
    "    df = pd.merge(df, df_grouped, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2199aec4-99ab-42ec-a729-cc0e442be274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check of percentage columns\n",
    "df['%MEGYE_2016'].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd2713-ae09-492d-8044-e87ad0eab7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check of percentage columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e082bfd8-cfb3-42ce-8fec-69e79d27accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check of percentage\n",
    "df[['MEGYE_2014', '%MEGYE_2014']].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399fd34-a017-4dc0-8990-5012869af9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage by count on alakul_ev\n",
    "col = 'alakul_ev'\n",
    "\n",
    "# calculate the percentage \n",
    "df_grouped = df[col].value_counts(normalize = True)\n",
    "# get the index as column\n",
    "df_grouped = df_grouped.reset_index()\n",
    "# change the column names :: orig column and percent\n",
    "percent_col = '%'+col\n",
    "df_grouped.columns = [col, percent_col]\n",
    "# merge the percent values to orig df\n",
    "df = pd.merge(df, df_grouped, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca162f4d-df30-4cb3-a14c-7a3f8a9ef563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check alakul ev percentage\n",
    "df[['alakul_ev', '%alakul_ev']].sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba66cc-cda2-4fab-b9b9-c5cec464b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualise target by categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a132c-dba4-4776-bd1c-541e303f2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise KATEGORIKUS_VALTOZO\n",
    "fig, ax =plt.subplots(1,3)\n",
    "sns.countplot(y='KATEGORIKUS_VALTOZO_2014', data=df, ax=ax[0])\n",
    "sns.countplot(y='KATEGORIKUS_VALTOZO_2015', data=df, ax=ax[1])\n",
    "sns.countplot(y='KATEGORIKUS_VALTOZO_2016', data=df, ax=ax[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c558b45-a7f2-486f-a0c2-6219d7fd058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the target variable by categorical features\n",
    "\n",
    "for column in df.select_dtypes(include='object'):\n",
    " if df[column].nunique() < 21:\n",
    "     sns.boxplot(y=column, x='target_reg', data=data)\n",
    "     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78684c84-816b-4656-83cb-8548748554df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group numeric features by each categorical feature\n",
    "\n",
    "for column in df.select_dtypes(include='object'):\n",
    "    if df[column].nunique() == 20:\n",
    "        display(df.groupby(column).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6158976-cf72-4e09-ab72-4bdb4531635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns for percentage\n",
    "columns_obj_perc = [column for column in df.columns if \"%\" in column]\n",
    "print(\"Percentage columns:: \\n{} \".format(columns_obj_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f3b00-965a-47fa-9a50-134245a23689",
   "metadata": {},
   "source": [
    "# Distribution of numerical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5fa08-4e81-460d-afeb-5ad57031ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check\n",
    "numeric_cols_2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc76a20-1569-42bd-977d-96314db3c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise numeric columns distribution\n",
    "for col in numeric_cols_2014:\n",
    "    plt.figure()\n",
    "    #sns.distplot(df['ertek_2016'], hist=False, kde=True)\n",
    "    sns.distplot(df[col], hist=False, kde=True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc5008-5093-4e52-860b-53dd8958cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_reg distribution\n",
    "plt.figure()\n",
    "sns.distplot(np.log(df['target_reg']), hist=False, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf38720d-342e-4bfd-bf91-7a187a84fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which Scaler is the best\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, QuantileTransformer\n",
    "\n",
    "robust_features = RobustScaler(quantile_range=(5, 95)).fit_transform(df[numeric_cols_2014].values)\n",
    "#df[numeric_cols_2014] = pd.DataFrame(robust_features, index=df.index, columns=df[numeric_cols_2014].columns)\n",
    "\n",
    "quantile_features = QuantileTransformer(output_distribution=\"normal\").fit_transform(df[numeric_cols_2014].values)\n",
    "#df[numeric_cols_2014] = pd.DataFrame(quantile_features, index=df.index, columns=df[numeric_cols_2014].columns)\n",
    "\n",
    "scaled_features = StandardScaler().fit_transform(df[numeric_cols_2014].values)\n",
    "#df[numeric_cols_2014] = pd.DataFrame(scaled_features, index=df.index, columns=df[numeric_cols_2014].columns)\n",
    "\n",
    "# the best transformation method\n",
    "\n",
    "# numeric colums :: divided/letszam and categorical percentage columns\n",
    "df[numeric_cols_to_transform] = numeric_cols_2014 + numeric_cols_2015 + numeric_cols_2016 + columns_obj_perc\n",
    "\n",
    "power_features = PowerTransformer(method=\"yeo-johnson\",  standardize=True).fit_transform(df[numeric_cols_to_transform].values)\n",
    "df[numeric_cols_to_transform] = pd.DataFrame(power_features, index=df.index, columns=df[numeric_cols_to_transform].columns)\n",
    "\n",
    "\n",
    "col = ['target_reg']\n",
    "ptt = PowerTransformer(method=\"yeo-johnson\",  standardize=True)\n",
    "power_target_feature = ptt.fit_transform(df[col].values)\n",
    "print(\"Lamdas for target_reg :: {}\".format(ptt.lambdas_))\n",
    "df['transformed'] = pd.DataFrame(power_target_feature, index=df.index, columns=df[col].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248193a0-646f-47cf-899b-6fdf8d14031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lamdas for target_reg :: {}\".format(ptt.lambdas_))\n",
    "df['transformed'] = pd.DataFrame(power_target_feature, index=df.index, columns=df[col].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440961fb-87d3-4e38-aaa2-b7862961d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "numeric_cols = col + numeric_cols_to_transform\n",
    "tab_data(df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7f95c-19e3-44f8-ae72-0573888b255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick checks \n",
    "df_orig[numeric_cols_2014].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb7d237-5079-4181-96bb-57c6b432a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick checks\n",
    "df[numeric_cols_2014].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e987d-f157-42f6-b729-1b35c6b59dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the transformations\n",
    "for col in numeric_cols_2014:\n",
    "    #define plotting region (1 row, 2 columns)\n",
    "    fig, axes = plt.subplots(1, 3)\n",
    "\n",
    "    #create distplot in each subplot\n",
    "    sns.distplot(df_orig[col], hist=False, kde=True, ax=axes[0])\n",
    "    sns.distplot(np.log(df[col]), hist=False, kde=True, ax=axes[1])\n",
    "    sns.distplot(df[col], hist=False, kde=True, ax=axes[2])\n",
    "    #sns.distplot(power_features[col], hist=False, kde=True, ax=axes[3])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72e9c0-e92b-46e8-8cd0-596526b65b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate and multivariate boxplots for different categorical features\n",
    "\n",
    "for col in numeric_cols_2014:\n",
    "    #define plotting region (1 row, 2 columns)\n",
    "    fig, ax =plt.subplots(1,2)\n",
    "    sns.boxplot(x=df[col], ax=ax[0])\n",
    "    sns.boxplot(x='agazat_2016_fo_kategoria', y=df[col], data=df, ax=ax[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006aa76-1fa3-4242-8037-82323bb0db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple distribution plots for numerical features\n",
    "plt.figure()\n",
    "sns.distplot(np.log(df_orig['letszam_2014']), hist=False, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec9f096-f1b8-4b8d-8e64-1d927ae6e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check of the transformed target_reg data\n",
    "print(len(df[df['transformed'] <=0]))\n",
    "print(len(df[df['transformed'] ==0]))\n",
    "print(len(df[df['transformed'] >=0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589db69-f8e6-4f31-8fea-40b9e8475164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check of the transformed target_reg data\n",
    "df[df['transformed'] >=0].nsmallest(100, \"target_reg\")\n",
    "df[df['transformed'] >=0].nsmallest(100, \"transformed\")\n",
    "df[df['transformed'] >=0].nlargest(10, \"transformed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f2f645-5b56-468b-8190-fdcce3215f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check of the transformed target_reg data\n",
    "df[df['transformed'] <=0].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db8581-b828-498f-a66c-99ca864e06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairlots for two numeric features defining hue\n",
    "categorical_cols = ['KATEGORIKUS_VALTOZO_2016', 'agazat_2016_fo_kategoria']\n",
    "plt.figure()\n",
    "sns.pairplot(df[['sajattok_2016', 'transformed', 'agazat_2016_fo_kategoria']], \n",
    "             hue='agazat_2016_fo_kategoria') #hue='origin', kind='reg'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ac41c-f52a-45c5-8358-ac6fabfebb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median, quantiles\n",
    "percentages = [0.25, 0.55, 0.95]\n",
    "\n",
    "print(df[columns].describe(include='float64').median())\n",
    "df[columns].quantile(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb4962-ac11-4b31-962c-14bc16a448f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1ac91-fabb-4f6e-a333-60bd48a421f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate quantile ranges\n",
    "# inter quartile range - it's the distance between 75th and 25th percentiles\n",
    "def IQR(column): \n",
    "    q25, q75 = column.quantile([0.25, 0.75])\n",
    "    return q75-q25\n",
    "\n",
    "# lower_bound\n",
    "def lower_bound(column):\n",
    "    q25 = column.quantile(0.25)\n",
    "    q75 = column.quantile(0.75)\n",
    "    iqr = q75 - q25\n",
    "    lower_bound = q25 -(1.5 * iqr) \n",
    "    return lower_bound\n",
    "\n",
    "# Upper_bound\n",
    "def upper_bound(column):\n",
    "    q25 = column.quantile(0.25)\n",
    "    q75 = column.quantile(0.75)\n",
    "    iqr = q75 - q25\n",
    "    upper_bound = q75 + (1.5 * iqr) \n",
    "    return upper_bound\n",
    "\n",
    "# range - It's the difference between input's maximum and minimum values\n",
    "# range() is already a built-in function in Python - another name for custom function\n",
    "def range_f(column):\n",
    "    return column.max() - column.min()\n",
    "\n",
    "stats_list = [\n",
    "    'min', 'max', \n",
    "    range_f, # custom function \n",
    "    'mean', 'median',  'std',\n",
    "    np.var, # numpy function\n",
    "    IQR,   # custom function\n",
    "    lower_bound, # custom function\n",
    "    upper_bound, # custom function\n",
    "]\n",
    "\n",
    "summary_stats = df[columns].agg(func=stats_list)\n",
    "\n",
    "# custom names for the statistics - in the same order as in the 'stat_list' variable\n",
    "pretty_names = [\n",
    "    'Minimum', 'Maximum', 'Range', 'Mean', 'Median', \n",
    "    'Standard Deviation', 'Variance', 'IQR', 'Lower_bound', 'Upper_bound']\n",
    "\n",
    "# update the index labels with custom names\n",
    "summary_stats.index = pretty_names\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f90a4-aaee-4931-aa54-54148beafa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nr of outliers for different numeric columns\n",
    "\n",
    "print('Nr of lower_bound outliers :: {} '.format(len(df[df.log_ertek_2016 < 7])))\n",
    "print('Nr of upper_bound outliers :: {} '.format(len(df[df.log_ertek_2016 > 12])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee972ea9-1bb4-44b2-aefc-e87defe3282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate and multivariate boxplots for numeric columns\n",
    "fig, ax =plt.subplots(1,2)\n",
    "sns.boxplot(x=df['target_reg'], ax=ax[0])\n",
    "sns.boxplot(x='agazat_2016_fo_kategoria', y='target_reg', data=df, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c7d944-aaea-4373-8057-1d79214ee744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate and multivariate boxplots for numeric columns\n",
    "fig, ax =plt.subplots(1,2)\n",
    "sns.boxplot(x=df['arbevert_2014'], ax=ax[0])\n",
    "sns.boxplot(x='KATEGORIKUS_VALTOZO_2014', y='arbevert_2014', data=df, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ac0ab-5029-4152-81ca-09ac768da7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate and multivariate boxplots for numeric columns\n",
    "fig, ax =plt.subplots(1,2)\n",
    "sns.boxplot(x=df['target_reg'], ax=ax[0])\n",
    "sns.boxplot(x='KATEGORIKUS_VALTOZO_2015', y='target_reg', data=df, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4d10e2-a451-4113-bc18-c2282deefa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using IsolationForest\n",
    "\n",
    "https://towardsdatascience.com/anomaly-detection-with-isolation-forest-visualization-23cd75c281e2\n",
    "https://scikit-learn.org/stable/auto_examples/index.html\n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea9c46-b915-462f-a0b8-0819d895f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using IsolationForest\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])\n",
    "clf = IsolationForest(n_estimators=10, warm_start=True)\n",
    "clf.fit(X)  # fit 10 trees  \n",
    "clf.set_params(n_estimators=20)  # add 10 more trees  \n",
    "clf.fit(X)  # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc59248d-bce3-4d42-b9a8-6e5220ad99b6",
   "metadata": {},
   "source": [
    "# Create model df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93cabaf-7ead-4e6b-b656-8474cfc0f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model columns\n",
    "model_columns = ['transformed']+ ['KATEGORIKUS_VALTOZO_2014'] + ['KATEGORIKUS_VALTOZO_2015']+ ['KATEGORIKUS_VALTOZO_2016']+ \n",
    "                numeric_cols_2014 + numeric_cols_2015 + numeric_cols_2016 + columns_obj_perc\n",
    "\n",
    "df_model = df[model_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e30c48-9001-44e1-a5ad-34c9c2cef1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check for model columns\n",
    "print('NR of features :: {}'.format(len(df_model.columns)))\n",
    "\n",
    "df_model.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd28c56-987d-41f0-838d-342387b45f5c",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769aa10f-8720-4ca4-b530-6150853fa675",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = df.corr()\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4710207-81db-4eed-9e51-a29cc76af5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDf = dfCorr[((dfCorr >= .5) | (dfCorr <= -.5)) & (dfCorr !=1.000)]\n",
    "filteredDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30b819-9ed2-4864-83f5-d112de99903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of the correlations\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corrs, cmap='RdBu_r', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21526dc-203d-4f52-b321-a3e6658c2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df for correlation\n",
    "dfCorr = df.corr()\n",
    "filteredDf = dfCorr[((dfCorr >= .7) | (dfCorr <= -.7)) & (dfCorr !=1.000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec3a59-e6a8-4626-aa4f-bbcb33f59abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise correlation\n",
    "plt.figure(figsize=(30,10))\n",
    "sns.heatmap(filteredDf, annot=True, cmap=\"Reds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa72923-6b27-42e6-bbef-9b4fe3fb0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCorr = df.corr().unstack().sort_values().drop_duplicates()\n",
    "dfCorr[((dfCorr >= .8) | (dfCorr <= -.8)) & (dfCorr !=1.000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c6cbd0-3238-4594-8b11-c8a1b9d71688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1257ca-e969-4631-acc5-8e4feef14407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015.sample(1)\n",
    "df_2016.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af774d6-3e28-411e-a19b-1250bd239b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation for 2016\n",
    "dfCorr = df_2016.corr().unstack().sort_values().drop_duplicates()\n",
    "dfCorr[((dfCorr >= .8) | (dfCorr <= -.8)) & (dfCorr !=1.000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183bd23d-fa9c-4530-8cdf-3a04a5110398",
   "metadata": {},
   "source": [
    "# xgboost basic model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd5571-de2b-4cd0-b698-2f058b8f002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "import xgboost as xgb\n",
    "print(xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6f6cb-d2c0-435b-ae8e-c5c8640da2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an xgboost basic regression model\n",
    "model = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "206f5c63-6a2e-47b0-9699-c28a1b17b253",
   "metadata": {},
   "source": [
    "# parameters of the model\n",
    "Perhaps the most commonly configured hyperparameters are the following:\n",
    "\n",
    "n_estimators: The number of trees in the ensemble, often increased until no further improvements are seen.\n",
    "max_depth: The maximum depth of each tree, often values are between 1 and 10.\n",
    "eta: The learning rate used to weight each model, often set to small values such as 0.3, 0.1, 0.01, or smaller.\n",
    "subsample: The number of samples (rows) used in each tree, set to a value between 0 and 1, often 1.0 to use all samples.\n",
    "colsample_bytree: Number of features (columns) used in each tree, set to a value between 0 and 1, often 1.0 to use all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02041363-9c5a-4a8d-850a-c091cbf1a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an xgboost regression model by configuring hyperparameters \n",
    "model = xgb.XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=1, colsample_bytree=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007b6e42-ec77-464a-9b3f-974d6e0a3301",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016.columns\n",
    "df.select_dtypes(include='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915b4f7-ff77-4407-a2a2-5c906b235939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into input and output columns\n",
    "X, y = df_2016.iloc[:, 2:len(df_2016)], df_2016.iloc[:, 1]\n",
    "X = X.select_dtypes(include='float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686f67b-139d-41aa-89df-173677346f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8338d0-f217-4271-9566-e91f76b568e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# force scores to be positive\n",
    "scores = np.absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b4f142-2a99-4254-98f2-55996aaebfb9",
   "metadata": {},
   "source": [
    "# xgboost basic model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620d7bb-1235-467f-80a5-8ccce2ddf798",
   "metadata": {},
   "source": [
    "## Exhaustively search for the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1069120-7306-4a69-8fe3-22e31780804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "print(xgb.__version__)\n",
    "\n",
    "#=========================================================================\n",
    "# XGBoost regression: \n",
    "# Parameters: \n",
    "# n_estimators  \"Number of gradient boosted trees. Equivalent to number \n",
    "#                of boosting rounds.\"\n",
    "# learning_rate \"Boosting learning rate (also known as “eta”)\"\n",
    "# max_depth     \"Maximum depth of a tree. Increasing this value will make \n",
    "#                the model more complex and more likely to overfit.\" \n",
    "#=========================================================================\n",
    "regressor=xgb.XGBRegressor(eval_metric='rmse')\n",
    "\n",
    "#=========================================================================\n",
    "# exhaustively search for the optimal hyperparameters\n",
    "#=========================================================================\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# set up our search grid\n",
    "param_grid = {\"max_depth\":    [4, 5,8],\n",
    "              \"n_estimators\": [101, 207, 700],\n",
    "              \"learning_rate\": [0.001, 0.01, 0.015]}\n",
    "\n",
    "# try out every combination of the above values\n",
    "search = GridSearchCV(regressor, param_grid, cv=5).fit(X, y)\n",
    "\n",
    "print(\"The best hyperparameters are \",search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faabdc48-adb3-45fa-9e2f-4171de1aabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the calculated parameters\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train and test split\n",
    "X, y = df_model.iloc[:, 2:len(df_model)], df_model.iloc[:, 1]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2)\n",
    "\n",
    "regressor=xgb.XGBRegressor(learning_rate = search.best_params_[\"learning_rate\"],\n",
    "                           n_estimators  = search.best_params_[\"n_estimators\"],\n",
    "                           max_depth     = search.best_params_[\"max_depth\"],\n",
    "                           eval_metric='rmse')\n",
    "\n",
    "regressor.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "predictions = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cda161-1eef-4b0f-b116-5a97aa8515c8",
   "metadata": {},
   "source": [
    "## Without GridSearch using test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca27eba-0ddd-4f7d-9a14-d2300986ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# print(xgb.__version__)\n",
    "\n",
    "regressor=xgb.XGBRegressor(eval_metric='rmse')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into input and output columns\n",
    "X, y = df_model.iloc[:, 2:len(df_model)], df_model.iloc[:, 1]\n",
    "\n",
    "#X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2)\n",
    "\n",
    "regressor=xgb.XGBRegressor(objective ='reg:squarederror',\n",
    "                          learning_rate = 0.001,\n",
    "                           n_estimators  = 100,\n",
    "                           max_depth     = 35,\n",
    "                           alpha = 10,\n",
    "                           eval_metric='rmse')\n",
    "\n",
    "regressor.fit(X, y, verbose=True)\n",
    "\n",
    "predictions = regressor.predict(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e01cc-d0ee-45f6-967e-ce50e58fbd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MAE, MSE - not having target_reg trz using arbevetexp_2016 as the most closeset data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mae = np.sqrt(mean_absolute_error(df_test['arbevetexp_2016'], predictions))\n",
    "print(\"MAE: %f\" % (mae))\n",
    "mse = np.sqrt(mean_squared_error(df_test['arbevetexp_2016'], predictions))\n",
    "print(\"MSE: %f\" % (mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29d13dc-d954-4c9a-aab5-c71ffc535e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[200:203]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65511f-ae89-4b98-bafa-87faecbe4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred= pd.DataFrame(predictions)\n",
    "df_test['prediction'] = df_pred\n",
    "\n",
    "df_test['inv_pred'] = pd.DataFrame(pt.inverse_transform(df_pred.values.reshape(-1, 1)))\n",
    "df_test['inv_pred'] = df_test.inv_pred*df_test_orig.letszam_2016\n",
    "\n",
    "df_test['arbevexp_inv'] = pd.DataFrame(pt.inverse_transform(df_test.arbevexp_2016.values.reshape(-1, 1)))\n",
    "df_test['arbevexp_inv'] = df_test.arbevexp_inv*df_test_orig.letszam_2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d9defb-a8c9-4888-9014-ca442cc5a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['prediction','inv_pred', 'arbevexp_2016', 'arbevexp_inv']][200:203]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0cca9d-0021-4ed4-b503-e2f2834a04e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_orig[['arbevexp_2016', 'letszam_2016']][200:203]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588dda36-3742-4dcc-84f7-2d1203b39d6b",
   "metadata": {},
   "source": [
    "## With GridSearch by 80-20 rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded68a9-2a37-40af-aaf9-656cf749a729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#print(xgb.__version__)\n",
    "\n",
    "regressor=xgb.XGBRegressor(eval_metric='rmse')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into input and output columns\n",
    "X, y = df_model.iloc[:, 2:len(df_model)], df_model.iloc[:, 1]\n",
    "\n",
    "# split data into train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2)\n",
    "\n",
    "regressor=xgb.XGBRegressor(objective ='reg:squarederror',\n",
    "                          learning_rate = 0.001,\n",
    "                           n_estimators  = 100,\n",
    "                           max_depth     = 35,\n",
    "                           alpha = 10,\n",
    "                           eval_metric='rmse')\n",
    "\n",
    "regressor.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "predictions = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3505b66-ea4b-46c3-811b-032824b6bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MAE, MSE\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mae = np.sqrt(mean_absolute_error(y_test, predictions))\n",
    "print(\"MAE: %f\" % (mae))\n",
    "mse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(\"MSE: %f\" % (mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694220f8-84b3-44a4-93ed-65e621d34f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[11:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99801a-d133-4767-a412-addf2e4134a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[11:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88991698-eebd-4eea-9412-3827d21b4257",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred= pd.DataFrame(predictions)\n",
    "df['prediction'] = df_pred\n",
    "df['inv_pred'] = pd.DataFrame(ptt.inverse_transform(df_pred.values.reshape(-1, 1)))\n",
    "#df['inv_pred'] = df.inv_pred*df.letszam_2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9793de-6691-4453-bdb2-c49e81b0e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e89a9-99f0-42bf-a2fc-bb5f4a9cb63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MAE, MSE\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mae = np.sqrt(mean_absolute_error(df_['target_reg'], df_['inv_pred']))\n",
    "print(\"MAE: %f\" % (mae))\n",
    "mse = np.sqrt(mean_squared_error(df_['target_reg'], df_['inv_pred']))\n",
    "print(\"MSE: %f\" % (mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f1afbb-e0d0-42fe-a091-87e8551f98b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['inverse'] = pd.DataFrame(ptt.inverse_transform(df['transformed'].values.reshape(-1, 1)))\n",
    "#df['inverse'] = df.inverse*df.letszam_2016\n",
    "df['arbevexp_inv'] = pd.DataFrame(pt.inverse_transform(df['arbevexp_2016'].values.reshape(-1, 1)))\n",
    "#df['arbevexp_inv'] = df.arbevexp_inv*df.letszam_2016\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb0ca8-5a98-4b73-bd52-39af3b4f215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[ 'target_reg', 'transformed', 'inverse','prediction', 'inv_pred', 'arbevexp_2016', 'arbevexp_inv', 'letszam_2016']][11:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf7aa4-9ebf-41a2-96c9-eb66d13ad4c2",
   "metadata": {},
   "source": [
    "## Visualise tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf10d5e-8271-4648-a0c1-27c74bef8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6780c90a-1b41-4d23-b20a-1adb58988ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d9a68e-f995-44b7-819e-6bb86e8c1ce7",
   "metadata": {},
   "source": [
    "# Miscellous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1d721-78cb-4ba8-b4b0-8237271f74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['arbevert_2014', 'arbevert_2015', 'arbevert_2016']].hist(figsize=(14,14), xrot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277266ac-cda4-4c9a-8f32-d33ad3eac152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 'alakul_ev' use the most frequent values \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "df['alakul_ev'] = imp.fit_transform(df['alakul_ev'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca195c86-e631-48d4-8a1d-4a55355c05c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation, multi-index\n",
    "\n",
    "df.groupby(\"quality\").agg({\n",
    "    \"fixed acidity\": [\"sum\", \"mean\", \"std\"],\n",
    "    \"volatile acidity\": [\"sum\", \"mean\", \"std\"]\n",
    "})\n",
    "\n",
    "#You will notice that the column names have been put on two levels. This is called a MultiIndex and works a little differently. This is looked at in more detail in another guide, but here are a couple of quick tips:\n",
    "\n",
    "df[\"fixed acidity\"] # will now return a DataFrame with the three columns under fixed acidity.\n",
    "df[(\"fixed acidity\", \"sum\")] # will select the specific fixed acidity sum column.\n",
    "\n",
    "#If you just want to flatten the index into one level, you can do the following:\n",
    "\n",
    "df_grouped.columns = df_grouped.columns.to_flat_index()\n",
    "df_grouped\n",
    "\n",
    "#  drop one level index\n",
    "df_grouped.droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a692410a-c7bb-4f8c-8883-477226a5de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the following syntax to calculate the percentage of a total within groups in pandas:\n",
    "df['values_var'] / df.groupby('group_var')['values_var'].transform('sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f1546-e588-4010-b8c3-647c295d76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc dealing with column names\n",
    "cols_no_year = ['id', 'target_reg', 'alakul_ev']\n",
    "\n",
    "columns_2014_obj = [column for column in df.select_dtypes(include='object').columns if \"2014\" in column]\n",
    "\n",
    "columns_2015 = [column for column in df.columns if \"2015\" in column]\n",
    "columns_2016 = [column for column in df.columns if \"2016\" in column]\n",
    "\n",
    "columns = cols_no_year + columns_2014\n",
    "columns_2014_obj.remove(['GAZDALKODASI_FORMA_2014_1_alkategoria'])\n",
    "\n",
    "# if \"2014\" in column\n",
    "obj = [column for column in df.select_dtypes( include=['object']).columns if \"2014\" in column]\n",
    "obj\n",
    "num = [column for column in df.select_dtypes(exclude=['int64'], include=['float64']).columns if \"2014\" in column]\n",
    "#num = num.remove(['KATEGORIKUS_VALTOZO_2014', 'letszam_2014'])\n",
    "num = [item for item in num if item not in  ['KATEGORIKUS_VALTOZO_2014', 'letszam_2014']]\n",
    "num\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc900da-6341-45aa-825e-9c42475c23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess categorical columns - transform them in percentage ones\n",
    "df_ = df.copy()\n",
    "\n",
    "for col in columns_2014_obj:\n",
    "    # calculate the percentage \n",
    "    df_grouped = df_[col].value_counts(normalize = True)\n",
    "    # get the index as column\n",
    "    df_grouped = df_grouped.reset_index()\n",
    "    # change the column names :: orig column and percent\n",
    "    percent_col = '%'+col\n",
    "    df_grouped.columns = [col, percent_col]\n",
    "    # merge the percent values to orig df\n",
    "    df_ = pd.merge(df_, df_grouped, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c02ac-d03f-4d2a-a6a8-c7229ce3a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add percentage column - 1 column\n",
    "df_ = df.copy()\n",
    "\n",
    "col = 'MEGYE_2014'\n",
    "\n",
    "# calculate the percentage \n",
    "df_grouped = df_[col].value_counts(normalize = True)\n",
    "# get the index as column\n",
    "df_grouped = df_grouped.reset_index()\n",
    "# change the column names :: orig column and percent\n",
    "percent_col = '%'+col\n",
    "df_grouped.columns = [col, percent_col]\n",
    "# merge the percent values to orig df\n",
    "df_ = pd.merge(df_, df_grouped, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf54611-daeb-439f-bd75-d1558028ce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove alkategoriak\n",
    "to_be_removed_list = ['GAZDALKODASI_FORMA_2014_1_alkategoria', 'GAZDALKODASI_FORMA_2014_2_alkategoria', \n",
    "                      'KKV_BESOROLAS_2014_1_alkategoria','KKV_BESOROLAS_2014_2_alkategoria',\n",
    "                      'agazat_2014_1_alkategoria','agazat_2014_2_alkategoria','agazat_2014_3_alkategoria',\n",
    "                      \n",
    "                      'GAZDALKODASI_FORMA_2015_1_alkategoria', 'GAZDALKODASI_FORMA_2015_2_alkategoria', \n",
    "                      'KKV_BESOROLAS_2015_1_alkategoria','KKV_BESOROLAS_2015_2_alkategoria',\n",
    "                      'agazat_2015_1_alkategoria','agazat_2015_2_alkategoria','agazat_2015_3_alkategoria',\n",
    "                      \n",
    "                      'GAZDALKODASI_FORMA_2016_1_alkategoria', 'GAZDALKODASI_FORMA_2016_2_alkategoria', \n",
    "                      'KKV_BESOROLAS_2016_1_alkategoria','KKV_BESOROLAS_2016_2_alkategoria',\n",
    "                      'agazat_2016_1_alkategoria','agazat_2016_2_alkategoria','agazat_2016_3_alkategoria'\n",
    "                     ]\n",
    "\n",
    "columns_obj = [item for item in columns_obj_all if item not in to_be_removed_list]\n",
    "print(\"Required columns:: \\n{}\".format( columns_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5106ab5-d375-492d-ae58-a0c222183f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.log(ertek_2016)\n",
    "df['log_ertek_2016'] = np.log(df['ertek_2016'])\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(df['log_ertek_2016'], hist=False, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efcba9c-ccf4-4716-a0d9-a7df6e8357f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the outliers\n",
    "\n",
    "def calculateIQR(x):\n",
    "    q1 = df.quantile(0.25)\n",
    "    q3 = df.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 -(1.5 * iqr) \n",
    "    upper_bound = q3 +(1.5 * iqr)\n",
    "    return iqr, lower_bound, upper_bound\n",
    "\n",
    "#calculate IQR for all columns\n",
    "#result = df[columns].apply(calculateIQR)\n",
    "#print('\\nThe Interquartile Range for all columns are as follows:\\n',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33756dc6-799d-4e96-b3ae-75ac90d7f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform target_reg with box-cox\n",
    "from scipy.stats import boxcox \n",
    "from scipy.stats import boxcox, zscore\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# set 0s to 0.1 for boxcox to work\n",
    "df['target_reg_'] = np.where(df.target_reg>0, df.target_reg,0.01)\n",
    "print(len(df[df.target_reg_ ==0]))\n",
    "\n",
    "#perform Box-Cox transformation on original data\n",
    "df['transformed'], best_lambda = boxcox(df['target_reg_']) \n",
    "print(\"Best_lambda {}\".format(best_lambda))\n",
    "\n",
    "# Histogram and kernel density estimate\n",
    "plt.figure()\n",
    "sns.distplot(df['transformed'], hist=False, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f6a9f-a66b-4578-97f1-ceb1b373169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check\n",
    "columns = ['target_reg','transformed']\n",
    "df[columns].describe(include='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f0e88-24c2-4366-8227-22ca24022e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs per years\n",
    "def year_columns(year):\n",
    "   year_list = 'columns' + year\n",
    "   year_list = [column for column in df.columns if year in column]\n",
    "   return year_list\n",
    "\n",
    "df_2014 = df[cols_no_year + year_columns('2014')]\n",
    "df_2015 = df[cols_no_year + year_columns('2015')]\n",
    "df_2016 = df[cols_no_year + year_columns('2016')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
